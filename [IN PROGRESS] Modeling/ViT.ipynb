{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import ViTModel, ViTImageProcessor\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HouseImagesDataset(Dataset):\n",
    "    def __init__(self, root_dir, processor):\n",
    "        \"\"\"\n",
    "        root_dir: Directory with Georgetown property images.\n",
    "        processor: ViTImageProcessor for preprocessing (resize, normalization, etc.).\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.processor = processor\n",
    "        self.image_paths = [\n",
    "            os.path.join(root_dir, f) \n",
    "            for f in os.listdir(root_dir) \n",
    "            if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Preprocess the image for ViT\n",
    "        encoding = self.processor(images=image, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()  # shape: (3, H, W)\n",
    "        \n",
    "        return pixel_values, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(root_dir, processor, batch_size=8):\n",
    "    dataset = HouseImagesDataset(root_dir, processor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vit_model():\n",
    "    # Load the base ViT model (without the classification head)\n",
    "    model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "    model.eval()  # Set to eval mode for inference\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model, dataloader, device):\n",
    "    \"\"\" Extract embeddings for all images in `dataloader` using `model`. \n",
    "        Returns a list of (embedding, img_path).\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    all_embeddings = []\n",
    "    all_paths = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for pixel_values, img_paths in dataloader:\n",
    "            pixel_values = pixel_values.to(device)\n",
    "            outputs = model(pixel_values)\n",
    "            \n",
    "            # outputs.last_hidden_state is [batch_size, seq_len, hidden_size]\n",
    "            # The [CLS] token embedding is typically at index 0\n",
    "            # Some ViT models also have a `pooler_output`, but not all.\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # shape: (batch_size, hidden_size)\n",
    "\n",
    "            cls_embeddings = cls_embeddings.cpu().numpy()\n",
    "            \n",
    "            for emb, path in zip(cls_embeddings, img_paths):\n",
    "                all_embeddings.append(emb)\n",
    "                all_paths.append(path)\n",
    "\n",
    "    return np.array(all_embeddings), all_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_embeddings(embeddings, n_clusters=5):\n",
    "    \"\"\"\n",
    "    Use KMeans to cluster embeddings into `n_clusters`.\n",
    "    Returns the cluster assignments for each embedding.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1. Data directory with unlabeled images\n",
    "    root_dir = \"/path/to/house/images\"  # e.g. dataset/\n",
    "\n",
    "    # 2. Load ViT processor\n",
    "    processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "    # 3. Create data loader\n",
    "    batch_size = 8\n",
    "    dataloader = create_dataloader(root_dir, processor, batch_size)\n",
    "\n",
    "    # 4. Load the pretrained ViT model (for embeddings)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vit_model = load_vit_model()\n",
    "\n",
    "    # 5. Extract embeddings\n",
    "    embeddings, paths = extract_embeddings(vit_model, dataloader, device)\n",
    "\n",
    "    # 6. Cluster embeddings\n",
    "    n_clusters = 5  # you can guess how many clusters might exist\n",
    "    cluster_labels = cluster_embeddings(embeddings, n_clusters)\n",
    "\n",
    "    # 7. Inspect results\n",
    "    # Here, we just print out each image path with its cluster assignment\n",
    "    for img_path, label in zip(paths, cluster_labels):\n",
    "        print(f\"{img_path} -> Cluster {label}\")\n",
    "\n",
    "    # Optionally, you could group these by cluster and analyze images together\n",
    "    # to see if each cluster corresponds to a certain type of property, style, etc.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
